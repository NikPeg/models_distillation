# Дистилляция и ускорение моделей

Проект по дистилляции LLM под микропроцессор

## Структура проекта

- **Theory/** - теоретические материалы курса
  - `01-intro.md` - введение в дистилляцию и ускорение моделей
  - `02-knowledge-distill.md` - дистилляция знаний

- **Homework/** - домашние задания
  - `HW1/` - ДЗ №1: Анализ Z80-μLM и портирование на Arduino
    - `report.md` - основной отчёт с анализом
    - `experiments/` - код для экспериментов
  - `HW2/` - ДЗ №2: Оптимизация модели для Arduino UNO
    - `REPORT.md` - отчёт с результатами экспериментов
    - `experiments/` - скрипты обучения и бенчмарки
    - `code/` - реализация методов оптимизации
    - `results/` - сохранённые модели и метрики
  - `task.md` - общее описание заданий курса

- **z80ai/** - базовая модель [Z80-μLM](https://github.com/HarryR/z80ai)
  - 2-bit quantized language model для 8-bit процессора
  - Используется как baseline для исследований

## Описание курса

Курс посвящен методам дистилляции и ускорения ML-моделей для работы на ограниченных вычислительных ресурсах.

### Основные темы:

1. **Анализ вычислительной сложности моделей**
   - Профилирование моделей
   - Выявление bottlenecks
   - Оценка требований к ресурсам

2. **Дистилляция знаний**
   - Концепция Teacher-Student
   - Жесткие и мягкие метки
   - Передача знаний через скрытые слои

3. **Методы оптимизации**
   - Квантование
   - Pruning
   - Архитектурные оптимизации
   - Mixed precision
   - Kernel fusion

4. **Инженерные компромиссы**
   - Производительность vs точность
   - Память vs скорость
   - Сложность внедрения

## Домашнее задание №1

Анализ Z80-μLM (conversational AI для 8-bit процессора) и исследование возможности портирования на Arduino UNO/LilyPad с 2 KB SRAM.

**Гипотеза**: можно ли применить подход 2-bit quantization на платформах с 32x меньше памяти?

**План**:
1. Запуск baseline на Z80 эмуляторе
2. Обучение micro-модели для Arduino
3. Измерение trade-offs: скорость vs качество
4. Теоретический анализ: качество через offloading (SD/Flash) за счёт жертвы latency

Детали в `Homework/HW1/`

## Домашнее задание №2

Практическая реализация методов оптимизации для адаптации модели tinychat под микроконтроллер Arduino UNO (32 KB Flash, 2 KB SRAM).

**Задача**: оптимизировать модель z80ai/tinychat (256→256→192→128→39, 145K параметров) для запуска на Arduino.

**Реализованные методы**:
1. **Structured Pruning** - удаление 50% нейронов по важности (L1-норма весов)
2. **Binary Quantization** - квантование весов в 1-bit {-1, +1}
3. **Combined Pipeline** - последовательное применение методов

**Результаты**:

| Модель | Accuracy | Parameters | Flash | Latency | Arduino? |
|--------|----------|------------|-------|---------|----------|
| Baseline | 40.6% | 144,871 | 36.6 KB | 559 ms | ❌ |
| Pruned | **42.0%** | 54,023 | 13.8 KB | 208 ms | ✅ |
| Binary | 11.8% | 54,023 | 13.8 KB | 208 ms | ✅ |

**Ключевые достижения**:
- ✅ Baseline не влезает в Arduino (>32 KB)
- ✅ Pruned модель **улучшила** качество на +1.4% при сокращении размера до 37%
- ✅ Ускорение в 2.7× для оптимизированных моделей
- ✅ Grid search нашёл оптимальные гиперпараметры (lr=0.0001)

**Инструменты**:
- `experiments/train_real.py` - обучение baseline на реальных данных
- `experiments/quick_grid_search.py` - автоматический подбор гиперпараметров (15 запусков)
- `experiments/pipeline_real.py` - полный цикл оптимизации
- `experiments/benchmark_real.py` - сравнение моделей
- `experiments/chat.py` - интерактивный чат с моделью

Детали в `Homework/HW2/REPORT.md`

## Целевая аудитория

Студенты и инженеры, работающие с ML-моделями и желающие оптимизировать их для production-среды.
