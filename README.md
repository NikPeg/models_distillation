# Дистилляция и ускорение моделей

Проект по дистилляции LLM под микропроцессор

## Структура проекта

- **Theory/** - теоретические материалы курса
  - `01-intro.md` - введение в дистилляцию и ускорение моделей
  - `02-knowledge-distill.md` - дистилляция знаний

- **Homework/** - домашние задания
  - `HW1/` - ДЗ №1: Анализ Z80-μLM и портирование на Arduino
    - `report.md` - основной отчёт с анализом
    - `experiments/` - код для экспериментов
  - `task.md` - общее описание заданий курса

- **z80ai/** - базовая модель [Z80-μLM](https://github.com/HarryR/z80ai)
  - 2-bit quantized language model для 8-bit процессора
  - Используется как baseline для исследований

## Описание курса

Курс посвящен методам дистилляции и ускорения ML-моделей для работы на ограниченных вычислительных ресурсах.

### Основные темы:

1. **Анализ вычислительной сложности моделей**
   - Профилирование моделей
   - Выявление bottlenecks
   - Оценка требований к ресурсам

2. **Дистилляция знаний**
   - Концепция Teacher-Student
   - Жесткие и мягкие метки
   - Передача знаний через скрытые слои

3. **Методы оптимизации**
   - Квантование
   - Pruning
   - Архитектурные оптимизации
   - Mixed precision
   - Kernel fusion

4. **Инженерные компромиссы**
   - Производительность vs точность
   - Память vs скорость
   - Сложность внедрения

## Домашнее задание №1

Анализ Z80-μLM (conversational AI для 8-bit процессора) и исследование возможности портирования на Arduino UNO/LilyPad с 2 KB SRAM.

**Гипотеза**: можно ли применить подход 2-bit quantization на платформах с 32x меньше памяти?

**План**:
1. Запуск baseline на Z80 эмуляторе
2. Обучение micro-модели для Arduino
3. Измерение trade-offs: скорость vs качество
4. Теоретический анализ: качество через offloading (SD/Flash) за счёт жертвы latency

Детали в `Homework/HW1/`

## Целевая аудитория

Студенты и инженеры, работающие с ML-моделями и желающие оптимизировать их для production-среды.
