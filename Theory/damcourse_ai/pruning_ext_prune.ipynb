{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Усечение модели\n",
    "\n",
    "Продвинутый пример. Продолжение.\n",
    "\n",
    "## Интерактивное усечение и дообучение модели\n",
    "\n",
    "Перед запуском этого примера выполните предварительное обучение модели (pruning_ext_pretrain.ipynb)\n",
    "\n",
    "Оригинал стати можно найти [тут](https://leimao.github.io/blog/PyTorch-Pruning/).\n",
    "\n",
    "Этот пример лучше запускать либо на компьютере с GPU от NVidia, либо в [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "FcXBeP1O7cnY",
    "outputId": "2b081ee6-3006-47a5-8733-ea0c317bc78e"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNU-OD9O9ltP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Torch version {torch.__version__}\")\n",
    "\n",
    "# Let's make sure GPU is available!\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from utils import set_random_seeds, create_model, prepare_dataloader, train_model, save_model, load_model, evaluate_model, create_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_pruning_rate(pruning_rate, num_iterations):\n",
    "    \"\"\"A function to compute the final pruning rate for iterative pruning.\n",
    "        Note that this cannot be applied for global pruning rate if the pruning rate is heterogeneous among different layers.\n",
    "\n",
    "    Args:\n",
    "        pruning_rate (float): Pruning rate.\n",
    "        num_iterations (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        float: Final pruning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    final_pruning_rate = 1 - (1 - pruning_rate)**num_iterations\n",
    "\n",
    "    return final_pruning_rate\n",
    "\n",
    "\n",
    "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    if use_mask == True:\n",
    "        for buffer_name, buffer in module.named_buffers():\n",
    "            if \"weight_mask\" in buffer_name and weight == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "            if \"bias_mask\" in buffer_name and bias == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "    else:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name and weight == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "            if \"bias\" in param_name and bias == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n",
    "\n",
    "\n",
    "def measure_global_sparsity(model,\n",
    "                            weight=True,\n",
    "                            bias=False,\n",
    "                            conv2d_use_mask=False,\n",
    "                            linear_use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n",
    "\n",
    "\n",
    "def iterative_pruning_finetuning(model,\n",
    "                                 train_loader,\n",
    "                                 test_loader,\n",
    "                                 device,\n",
    "                                 learning_rate,\n",
    "                                 l1_regularization_strength,\n",
    "                                 l2_regularization_strength,\n",
    "                                 learning_rate_decay=0.1,\n",
    "                                 conv2d_prune_amount=0.4,\n",
    "                                 linear_prune_amount=0.2,\n",
    "                                 num_iterations=10,\n",
    "                                 num_epochs_per_iteration=10,\n",
    "                                 model_filename_prefix=\"pruned_model\",\n",
    "                                 model_dir=\"saved_models\",\n",
    "                                 grouped_pruning=False):\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        print(\"Pruning and Finetuning {}/{}\".format(i + 1, num_iterations))\n",
    "\n",
    "        print(\"Pruning...\")\n",
    "\n",
    "        if grouped_pruning == True:\n",
    "            # Global pruning\n",
    "            # I would rather call it grouped pruning.\n",
    "            parameters_to_prune = []\n",
    "            for module_name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    parameters_to_prune.append((module, \"weight\"))\n",
    "            prune.global_unstructured(\n",
    "                parameters_to_prune,\n",
    "                pruning_method=prune.L1Unstructured,\n",
    "                amount=conv2d_prune_amount,\n",
    "            )\n",
    "        else:\n",
    "            for module_name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    prune.l1_unstructured(module,\n",
    "                                          name=\"weight\",\n",
    "                                          amount=conv2d_prune_amount)\n",
    "                elif isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module,\n",
    "                                          name=\"weight\",\n",
    "                                          amount=linear_prune_amount)\n",
    "\n",
    "        _, eval_accuracy = evaluate_model(model=model,\n",
    "                                          test_loader=test_loader,\n",
    "                                          device=device,\n",
    "                                          criterion=None)\n",
    "\n",
    "        classification_report = create_classification_report(\n",
    "            model=model, test_loader=test_loader, device=device)\n",
    "\n",
    "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "            model,\n",
    "            weight=True,\n",
    "            bias=False,\n",
    "            conv2d_use_mask=True,\n",
    "            linear_use_mask=False)\n",
    "\n",
    "        print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report)\n",
    "        print(\"Global Sparsity:\")\n",
    "        print(\"{:.2f}\".format(sparsity))\n",
    "\n",
    "        # print(model.conv1._forward_pre_hooks)\n",
    "\n",
    "        print(\"Fine-tuning...\")\n",
    "\n",
    "        train_model(model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    device=device,\n",
    "                    l1_regularization_strength=l1_regularization_strength,\n",
    "                    l2_regularization_strength=l2_regularization_strength,\n",
    "                    learning_rate=learning_rate * (learning_rate_decay**i),\n",
    "                    num_epochs=num_epochs_per_iteration)\n",
    "\n",
    "        _, eval_accuracy = evaluate_model(model=model,\n",
    "                                          test_loader=test_loader,\n",
    "                                          device=device,\n",
    "                                          criterion=None)\n",
    "\n",
    "        classification_report = create_classification_report(\n",
    "            model=model, test_loader=test_loader, device=device)\n",
    "\n",
    "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "            model,\n",
    "            weight=True,\n",
    "            bias=False,\n",
    "            conv2d_use_mask=True,\n",
    "            linear_use_mask=False)\n",
    "\n",
    "        print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report)\n",
    "        print(\"Global Sparsity:\")\n",
    "        print(\"{:.2f}\".format(sparsity))\n",
    "\n",
    "        model_filename = \"{}_{}.pt\".format(model_filename_prefix, i + 1)\n",
    "        model_filepath = os.path.join(model_dir, model_filename)\n",
    "        save_model(model=model,\n",
    "                   model_dir=model_dir,\n",
    "                   model_filename=model_filename)\n",
    "        model = load_model(model=model,\n",
    "                           model_filepath=model_filepath,\n",
    "                           device=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def remove_parameters(model):\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            try:\n",
    "                prune.remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                prune.remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            try:\n",
    "                prune.remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                prune.remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "random_seed = 1\n",
    "l1_regularization_strength = 0\n",
    "l2_regularization_strength = 1e-4\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 1\n",
    "\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model_dir = \"saved_models\"\n",
    "model_filename = \"resnet18_cifar10.pt\"\n",
    "model_filename_prefix = \"pruned_model\"\n",
    "pruned_model_filename = \"resnet18_pruned_cifar10.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "pruned_model_filepath = os.path.join(model_dir, pruned_model_filename)\n",
    "\n",
    "set_random_seeds(random_seed=random_seed)\n",
    "\n",
    "# Create an untrained model.\n",
    "model = create_model(num_classes=num_classes)\n",
    "\n",
    "# Load a pretrained model.\n",
    "model = load_model(model=model,\n",
    "                    model_filepath=model_filepath,\n",
    "                    device=cuda_device)\n",
    "\n",
    "train_loader, test_loader, classes = prepare_dataloader(\n",
    "    num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "_, eval_accuracy = evaluate_model(model=model,\n",
    "                                    test_loader=test_loader,\n",
    "                                    device=cuda_device,\n",
    "                                    criterion=None)\n",
    "\n",
    "classification_report = create_classification_report(\n",
    "    model=model, test_loader=test_loader, device=cuda_device)\n",
    "\n",
    "num_zeros, num_elements, sparsity = measure_global_sparsity(model)\n",
    "\n",
    "print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n",
    "print(\"Global Sparsity:\")\n",
    "print(\"{:.2f}\".format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iterative Pruning + Fine-Tuning...\")\n",
    "\n",
    "pruned_model = copy.deepcopy(model)\n",
    "\n",
    "# iterative_pruning_finetuning(\n",
    "#     model=pruned_model,\n",
    "#     train_loader=train_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     device=cuda_device,\n",
    "#     learning_rate=learning_rate,\n",
    "#     learning_rate_decay=learning_rate_decay,\n",
    "#     l1_regularization_strength=l1_regularization_strength,\n",
    "#     l2_regularization_strength=l2_regularization_strength,\n",
    "#     conv2d_prune_amount=0.3,\n",
    "#     linear_prune_amount=0,\n",
    "#     num_iterations=8,\n",
    "#     num_epochs_per_iteration=50,\n",
    "#     model_filename_prefix=model_filename_prefix,\n",
    "#     model_dir=model_dir,\n",
    "#     grouped_pruning=True)\n",
    "\n",
    "iterative_pruning_finetuning(\n",
    "    model=pruned_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=cuda_device,\n",
    "    learning_rate=learning_rate,\n",
    "    learning_rate_decay=learning_rate_decay,\n",
    "    l1_regularization_strength=l1_regularization_strength,\n",
    "    l2_regularization_strength=l2_regularization_strength,\n",
    "    conv2d_prune_amount=0.98,\n",
    "    linear_prune_amount=0,\n",
    "    num_iterations=1,\n",
    "    num_epochs_per_iteration=200,\n",
    "    model_filename_prefix=model_filename_prefix,\n",
    "    model_dir=model_dir,\n",
    "    grouped_pruning=True)\n",
    "\n",
    "# Apply mask to the parameters and remove the mask.\n",
    "remove_parameters(model=pruned_model)\n",
    "\n",
    "_, eval_accuracy = evaluate_model(model=pruned_model,\n",
    "                                    test_loader=test_loader,\n",
    "                                    device=cuda_device,\n",
    "                                    criterion=None)\n",
    "\n",
    "classification_report = create_classification_report(\n",
    "    model=pruned_model, test_loader=test_loader, device=cuda_device)\n",
    "\n",
    "num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model)\n",
    "\n",
    "print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n",
    "print(\"Global Sparsity:\")\n",
    "print(\"{:.2f}\".format(sparsity))\n",
    "\n",
    "save_model(model=pruned_model, model_dir=model_dir, model_filename=pruned_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1, 3, 32, 32),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave\n",
    "\n",
    "original_cpu_inference_latency = measure_inference_latency(model=model,\n",
    "    device=cpu_device,\n",
    "    input_size=(1, 3, 32, 32),\n",
    "    num_samples=100)\n",
    "pruned_cpu_inference_latency = measure_inference_latency(\n",
    "    model=pruned_model,\n",
    "    device=cpu_device,\n",
    "    input_size=(1, 3, 32, 32),\n",
    "    num_samples=100)\n",
    "\n",
    "original_cuda_inference_latency = measure_inference_latency(model=model,\n",
    "    device=cuda_device,\n",
    "    input_size=(1, 3, 32, 32),\n",
    "    num_samples=100)\n",
    "pruned_cuda_inference_latency = measure_inference_latency(\n",
    "    model=pruned_model,\n",
    "    device=cuda_device,\n",
    "    input_size=(1, 3, 32, 32),\n",
    "    num_samples=100)\n",
    "\n",
    "print(\"Original CPU Inference Latency: {:.2f} ms / sample\".format(\n",
    "    original_cpu_inference_latency * 1000))\n",
    "print(\"Pruned CPU Inference Latency: {:.2f} ms / sample\".format(\n",
    "    pruned_cpu_inference_latency * 1000))\n",
    "\n",
    "print(\"Original CUDA Inference Latency: {:.2f} ms / sample\".format(\n",
    "    original_cuda_inference_latency * 1000))\n",
    "print(\"Pruned CUDA Inference Latency: {:.2f} ms / sample\".format(\n",
    "    pruned_cuda_inference_latency * 1000))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HotdogOrNot.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
