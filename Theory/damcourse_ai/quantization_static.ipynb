{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Статическое квантование\n",
    "\n",
    "Простой и наглядный пример для демонстрации основных концепций статического квантования.\n",
    "\n",
    "Оригинал статьи с кодом можно найти [тут](https://ninjalabo.ai/blogs/pytorch_staticq.html).\n",
    "\n",
    "Это пример лучше запускать либо на компьютере с GPU от NVidia, либо в [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "FcXBeP1O7cnY",
    "outputId": "2b081ee6-3006-47a5-8733-ea0c317bc78e"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNU-OD9O9ltP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Torch version {torch.__version__}\")\n",
    "\n",
    "# Let's make sure GPU is available!\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "import torch\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс __Quantizer__ для квантизации модели PyTorch. Для экспериментов мспользуем датасет Imagenette2-320 и модель ResNet18. И для удобства воспользуемся алгоритмами из  Fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer():\n",
    "    def __init__(self, backend=\"x86\"):\n",
    "        self.qconfig = get_default_qconfig_mapping(backend)\n",
    "        torch.backends.quantized.engine = backend\n",
    "\n",
    "    def quantize(self, model, calibration_dls):\n",
    "        x, _ = calibration_dls.valid.one_batch()\n",
    "        model_prepared = prepare_fx(model.eval(), self.qconfig, x)\n",
    "        with torch.no_grad():\n",
    "            _ = [model_prepared(xb.to('cpu')) for xb, _ in calibration_dls.valid]\n",
    "\n",
    "        return model_prepared, convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE_320, data=Path.cwd()/'data')\n",
    "dls = ImageDataLoaders.from_folder(path, valid='val', item_tfms=Resize(224),\n",
    "                                   batch_tfms=Normalize.from_stats(*imagenet_stats))\n",
    "learn = vision_learner(dls, resnet18)\n",
    "model_prepared, qmodel = Quantizer(\"qnnpack\").quantize(learn.model, learn.dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При статическом квантовании масштабный коэффициент и сдвиг нуля для весов и активаций определяются после калибровки модели, но до вывода результатов. Мы используем квантование по тензорам, а это значит, что есть единый масштабный коэффициент и сдвиг нуля применяются равномерно ко всем элементам каждого тензора слоя.\n",
    "\n",
    "Модель __model_prepared__ запоминает диапазоны активаций по всему набору валидационных данных. Значит, мы можем посчитать масштабный коэффициент и сдвиг нуля.\n",
    "\n",
    "Для сохранения диапазонов активаций используется __HistogramObserver__. Этот элемент появляется в модели благодаря применению функций __prepare_fx__ и __convert_fx__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example activation quantization parameters\n",
    "for i in range(3):\n",
    "    attr = getattr(model_prepared, f\"activation_post_process_{i}\")\n",
    "    scale, zero_p = attr.calculate_qparams()\n",
    "    print(\"{}\\nScaling Factor: {}\\nZero Point: {}\\n\".format(attr, scale.item(), zero_p.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что слои Conv2d объединяются со слоями ReLU в квантованные слои QuantizedConvReLU2d.\n",
    "\n",
    "В PyTorch, чтобы избежать избыточных процессов квантования и деквантования между слоями, пакетная нормализация встраивается в предыдущий слой (свертывание пакетной нормализации), а слой ReLU объединяется со следующим за ним слоем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем внимательно на первый слой квантованной модели (Conv2d + ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = qmodel._modules['0']._modules['0']\n",
    "print(layer)\n",
    "print(\"Weight Scale: {}, Weight Zero Point: {}\".format(layer.weight().q_scale(),\n",
    "                                                       layer.weight().q_zero_point()))\n",
    "print(\"Output Scaling Factor: {}, Output Zero Point: {}\\n\".format(layer.scale, \n",
    "                                                                  layer.zero_point))\n",
    "\n",
    "print(\"Example weights:\", layer.weight()[0, 0, 0])\n",
    "print(\"In integer representation:\", layer.weight()[0, 0, 0].int_repr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запустим инференс квантованной модели и сравним выходы первого сверточного слоя с фактическим результатом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_input = None\n",
    "layer_output = None\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global layer_output, layer_input\n",
    "    layer_input = input\n",
    "    layer_output = output\n",
    "\n",
    "img = torch.rand([1, 3, 224, 224])\n",
    "hook = qmodel._modules['0']._modules['0'].register_forward_hook(hook_fn)\n",
    "output = qmodel(img)\n",
    "hook.remove()\n",
    "print(\"Example input:\", layer_input[0][0,0,0,:10].int_repr())\n",
    "print(\"Example output:\", layer_output[0,0,0,:10].int_repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quantize(x, qparams, itype):\n",
    "    xtype = torch.iinfo(itype)\n",
    "    return torch.clamp(torch.round(x / qparams[0]) + qparams[1], min=xtype.min, max=xtype.max)\n",
    "\n",
    "def dequantize(x, qparams):\n",
    "    return (x - qparams[1]) * qparams[0]\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "    return torch.tensor(col)\n",
    "\n",
    "# first use im2col, which is efficient way to perform Conv2d operation\n",
    "inp = im2col(img, 7, 7, 2, 3).float()\n",
    "# quantize input values using input scale and zero point\n",
    "inp = quantize(inp, [layer_input[0].q_scale(), layer_input[0].q_zero_point()], torch.uint8)\n",
    "# get quantized weights, weight scale and quantize biases\n",
    "w = qmodel._modules['0']._modules['0'].weight().int_repr().reshape(64, -1).float()\n",
    "sw = qmodel._modules['0']._modules['0'].weight().q_scale()\n",
    "b = quantize(qmodel._modules['0']._modules['0'].bias(),\n",
    "             [layer_input[0].q_scale() * sw, 0], torch.int32)\n",
    "b = b.reshape(1,64,1,1).detach()\n",
    "# calculate matmul in Conv2d and add biases\n",
    "out = (w @ (inp.T - layer_input[0].q_zero_point())).view(1,64,112,112) + b\n",
    "# dequantize, perform ReLU and quantize based on output scale and zero point\n",
    "out = out * sw * layer_input[0].q_scale()\n",
    "out = torch.relu(out)\n",
    "out = quantize(out, [layer_output.q_scale(), layer_output.q_zero_point()], torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(out, layer_output.int_repr().float())\n",
    "print(\"Output: \", out[0, 0, 0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HotdogOrNot.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
