# Дистилляция и ускорение моделей
## Дистилляция знаний

## ПЛАН ЗАНЯТИЯ

### Лекция по дистилляции знаний
- Концепция модель-учитель — модель-ученик
- Виды дистилляции знаний: метки, логиты, признаки, скрытые слои, особые случаи
- Выбор моделей учителя и ученика

### Семинар
- Передача знаний классификатора
- Пошаговая дистилляция знаний в PyTorch на примере свёрточной сети

---

## Концепция модель-учитель — модель-ученик

- **Teacher (учитель)** — большая, предварительно обученная модель
- **Student (ученик)** — меньшая, менее сложная модель, которую мы хотим обучить
- Поддержка специфических слоев на железе
- Объем параметров
- Быстродействие
- Функция потерь
- Потеря точности
- Дистилляция знаний, а не данных
- Множественные учителя
- Онлайн, оффлайн и самодистилляция

---

## Жесткие метки

- Передача ключевых знаний
- Фокус на наиболее важной информации
- Упрощение обучения
- Потеря информации о распределении вероятностей классов
- Потенциальное ухудшение разнообразия

---

## Мягкие метки

- Хинтоновская дистилляция
- Дивергенция Кульбака - Лейблера
- Полное распределение вероятностей
- Масштабирование температуры
- Ценная информация в неправильных ответах
- Передача неопределенности
- Более богатый сигнал

---

## Скрытые слои

- Передача знаний признаков
- Данные со скрытых слоёв
- Промежуточные связи
- Масштабирование данных
- Гиперпараметры

---

## Самодистилляция

---

## Выбор моделей учителя и ученика

### Teacher (учитель)
- Сложная большая модель
- Обучена на большом датасете
- Высокая точность после обучения
- Большая задержка
- Вычислительная сложность
- Учитывает возможности ученика

### Student (ученик)
- Малая легковесная модель
- Высокая эффективность
- Фокус на минимизации разницы предсказаний
- Малые требования к памяти
- Архитектура оптимизирована под целевое железо
- Учитывает объем знаний учителя

**Метод GRAdient Cross-validation Evaluation (GRACE) для LLM**

---

## Передача знаний классификатора

- Обучение учителя
- Обучение ученика без учителя
- Обучение ученика с дистилляцией знаний «мягких» меток
- Обучение ученика с дистилляцией знаний одномерного классификатора
- Обучение ученика с дистилляцией знаний глубокого слоя

---

## Пошаговая дистилляция знаний в PyTorch на примере сверточной сети

```python
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets

optimizer.zero_grad()

with torch.no_grad():
    teacher_logits = teacher(inputs)

student_logits = student(inputs)

soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)
soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)

soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)
label_loss = ce_loss(student_logits, labels)

loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss
loss.backward()
```

---

## Полезные ссылки

- https://arxiv.org/abs/2006.05525
- https://arxiv.org/abs/1503.02531
- https://habr.com/ru/articles/891284/
- https://medium.com/@weidagang/demystifying-knowledge-distillation-in-neural-networks-0f4c82c070ed
- https://arxiv.org/abs/1911.04252
- https://arxiv.org/abs/2511.02833
- https://cyberleninka.ru/article/n/sposoby-umensheniya-vychislitelnoy-slozhnosti-neyrosetevyh-yazykovyh-modeley
- https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html
