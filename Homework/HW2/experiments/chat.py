#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é tinychat.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 chat.py --model ../results/baseline_real.pt
    python3 chat.py --model ../results/quick_grid/run_04.pt
"""

import torch
import numpy as np
import argparse
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'code'))

from baseline_model import BaselineModel
from data_loader import TrigramEncoder, ContextEncoder


class ChatBot:
    """–ß–∞—Ç-–±–æ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
    
    def __init__(self, model_path: str, device: str = 'cpu'):
        self.device = torch.device(device)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        print(f"Loading model from {model_path}...")
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.model = BaselineModel(
            input_size=checkpoint['architecture']['input_size'],
            hidden_sizes=checkpoint['architecture']['hidden_sizes'],
            num_classes=checkpoint['architecture']['num_classes']
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        self.model = self.model.to(self.device)
        
        # Charset –∏ encoders
        self.charset = checkpoint['charset']
        self.char_to_idx = {c: i for i, c in enumerate(self.charset)}
        self.idx_to_char = {i: c for i, c in enumerate(self.charset)}
        
        self.query_encoder = TrigramEncoder(num_buckets=128)
        self.context_encoder = ContextEncoder(num_buckets=128, context_len=8)
        
        print(f"‚úì Model loaded successfully!")
        print(f"  Architecture: {checkpoint['architecture']}")
        print(f"  Charset: {self.charset}")
        if 'val_acc' in checkpoint:
            print(f"  Validation accuracy: {checkpoint['val_acc']:.2%}")
        print()
    
    def predict_next_char(self, query: str, context: str = "") -> tuple[str, float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–º–≤–æ–ª.
        
        Args:
            query: –≤—Ö–æ–¥–Ω–æ–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –∫–æ–Ω—Ç–µ–∫—Å—Ç (—É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –æ—Ç–≤–µ—Ç–∞)
        
        Returns:
            (predicted_char, confidence)
        """
        # Encode input
        query_vec = self.query_encoder.encode(query)
        context_vec = self.context_encoder.encode(context)
        input_vec = np.concatenate([query_vec, context_vec])
        
        # Predict
        with torch.no_grad():
            x = torch.tensor(input_vec, dtype=torch.float32).unsqueeze(0).to(self.device)
            logits = self.model(x, quant_temp=1.0)
            probs = torch.softmax(logits, dim=-1)
            
            top_prob, top_idx = torch.max(probs[0], dim=0)
            predicted_char = self.idx_to_char[top_idx.item()]
            confidence = top_prob.item()
        
        return predicted_char, confidence
    
    def generate_response(self, query: str, max_length: int = 20, 
                         temperature: float = 1.0, top_k: int = 5) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (autoregressive generation).
        
        Args:
            query: –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
            temperature: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (1.0 = –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
            top_k: –±—Ä–∞—Ç—å —Ç–æ–ø-k –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        response = ""
        context = ""
        
        for _ in range(max_length):
            # Encode input
            query_vec = self.query_encoder.encode(query)
            context_vec = self.context_encoder.encode(context)
            input_vec = np.concatenate([query_vec, context_vec])
            
            # Predict with temperature
            with torch.no_grad():
                x = torch.tensor(input_vec, dtype=torch.float32).unsqueeze(0).to(self.device)
                logits = self.model(x, quant_temp=1.0)
                
                # Apply temperature
                if temperature != 1.0:
                    logits = logits / temperature
                
                probs = torch.softmax(logits, dim=-1)
                
                # Top-k sampling –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                if top_k > 1:
                    top_probs, top_indices = torch.topk(probs[0], k=top_k)
                    top_probs = top_probs / top_probs.sum()  # Renormalize
                    
                    # Sample from top-k
                    sampled_idx = torch.multinomial(top_probs, 1).item()
                    char_idx = top_indices[sampled_idx].item()
                else:
                    # Greedy (argmax)
                    char_idx = torch.argmax(probs[0]).item()
                
                predicted_char = self.idx_to_char[char_idx]
            
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ EOS –∏–ª–∏ –ø—Ä–æ–±–µ–ª–µ –≤ –∫–æ–Ω—Ü–µ
            if predicted_char == '\x00':  # EOS
                break
            
            response += predicted_char
            context = response[-8:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 8 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è context
            
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            if len(response) > 3 and response[-1] in [' ', '!', '?']:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞
                if len(response) > 5:
                    break
        
        return response.strip()
    
    def chat(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç."""
        print("="*70)
        print("TINYCHAT - Interactive Chat with Neural Network")
        print("="*70)
        print()
        print("Commands:")
        print("  /help    - Show this help")
        print("  /quit    - Exit chat")
        print("  /stats   - Show model statistics")
        print("  Ctrl+C   - Exit chat")
        print()
        print("Type your message and press Enter to chat!")
        print("-"*70)
        print()
        
        conversation_history = []
        
        try:
            while True:
                # –ü–æ–ª—É—á–∏—Ç—å –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                try:
                    user_input = input("You: ").strip()
                except EOFError:
                    print("\n\nGoodbye! üëã")
                    break
                
                if not user_input:
                    continue
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥
                if user_input.startswith('/'):
                    if user_input == '/quit':
                        print("\nGoodbye! üëã")
                        break
                    elif user_input == '/help':
                        print("\nCommands:")
                        print("  /help    - Show this help")
                        print("  /quit    - Exit chat")
                        print("  /stats   - Show conversation stats")
                        print()
                        continue
                    elif user_input == '/stats':
                        print(f"\nConversation statistics:")
                        print(f"  Messages: {len(conversation_history)}")
                        print(f"  Model: {self.model.get_architecture_str()}")
                        print(f"  Parameters: {self.model.count_parameters():,}")
                        print()
                        continue
                    else:
                        print(f"Unknown command: {user_input}")
                        print("Type /help for available commands")
                        print()
                        continue
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                response = self.generate_response(
                    user_input,
                    max_length=15,
                    temperature=0.8,  # –ù–µ–º–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                    top_k=3           # Top-3 sampling
                )
                
                # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π, –ø–æ–ø—Ä–æ–±—É–µ–º greedy
                if not response:
                    response = self.generate_response(
                        user_input,
                        max_length=10,
                        temperature=1.0,
                        top_k=1
                    )
                
                # –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—É—Å—Ç–æ–π, –±–µ—Ä—ë–º –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–≤—ã–π —Å–∏–º–≤–æ–ª
                if not response:
                    char, conf = self.predict_next_char(user_input)
                    response = char
                
                print(f"Bot: {response}")
                print()
                
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏—é
                conversation_history.append({
                    'user': user_input,
                    'bot': response
                })
        
        except KeyboardInterrupt:
            print("\n\nChat interrupted. Goodbye! üëã")
        except Exception as e:
            print(f"\n\nError: {e}")
            import traceback
            traceback.print_exc()


def main():
    parser = argparse.ArgumentParser(description='Interactive chat with tinychat model')
    
    parser.add_argument('--model', type=str, default='../results/quick_grid/run_04.pt',
                       help='Path to model checkpoint (.pt file)')
    parser.add_argument('--device', type=str, default='cpu',
                       choices=['cpu', 'cuda'],
                       help='Device to run on')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    if not os.path.exists(args.model):
        print(f"Error: Model not found at {args.model}")
        print(f"\nAvailable models:")
        results_dir = os.path.join(os.path.dirname(__file__), '..', 'results')
        if os.path.exists(results_dir):
            for root, dirs, files in os.walk(results_dir):
                for f in files:
                    if f.endswith('.pt'):
                        rel_path = os.path.relpath(os.path.join(root, f), 
                                                  os.path.dirname(__file__))
                        print(f"  {rel_path}")
        sys.exit(1)
    
    # –°–æ–∑–¥–∞—Ç—å —á–∞—Ç-–±–æ—Ç
    bot = ChatBot(args.model, device=args.device)
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å —á–∞—Ç
    bot.chat()


if __name__ == '__main__':
    main()
